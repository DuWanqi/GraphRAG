"""
è®°å¿†å›¾è°± Web åº”ç”¨
åŸºäº Gradio æ„å»ºçš„ç”¨æˆ·ç•Œé¢
"""

import asyncio
from typing import Optional, List, Tuple
import gradio as gr

from src.config import get_settings
from src.llm import create_llm_adapter, get_available_providers, LLMRouter
from src.indexing import GraphBuilder, DataLoader
from src.retrieval import MemoirRetriever
from src.generation import LiteraryGenerator, PromptTemplates


# å…¨å±€å˜é‡
settings = get_settings()
retriever: Optional[MemoirRetriever] = None
generator: Optional[LiteraryGenerator] = None
current_provider: Optional[str] = None  # è·Ÿè¸ªå½“å‰ä½¿ç”¨çš„ provider


def init_components(provider: str = "gemini"):
    """åˆå§‹åŒ–ç»„ä»¶"""
    global retriever, generator, current_provider
    
    # å¦‚æœ provider ç›¸åŒä¸”ç»„ä»¶å·²åˆå§‹åŒ–ï¼Œåˆ™è·³è¿‡
    if provider == current_provider and retriever is not None and generator is not None:
        return f"âœ… å·²ä½¿ç”¨ {provider} æ¨¡å‹"
    
    try:
        llm_adapter = create_llm_adapter(provider=provider)
        retriever = MemoirRetriever(llm_adapter=llm_adapter)
        generator = LiteraryGenerator(llm_adapter=llm_adapter)
        current_provider = provider
        return f"âœ… åˆå§‹åŒ–æˆåŠŸï¼ä½¿ç”¨ {provider} æ¨¡å‹"
    except Exception as e:
        return f"âŒ åˆå§‹åŒ–å¤±è´¥: {str(e)}"


async def process_memoir_async(
    memoir_text: str,
    provider: str,
    style: str,
    temperature: float,
) -> Tuple[str, str, str]:
    """
    å¼‚æ­¥å¤„ç†å›å¿†å½•
    
    Returns:
        (ç”Ÿæˆçš„å†å²èƒŒæ™¯, æå–çš„ä¿¡æ¯, æ£€ç´¢ç»“æœ)
    """
    global retriever, generator, current_provider
    
    if not memoir_text.strip():
        return "è¯·è¾“å…¥å›å¿†å½•æ–‡æœ¬", "", ""
    
    # ç¡®ä¿ç»„ä»¶åˆå§‹åŒ–ï¼Œæˆ–åœ¨ provider å˜åŒ–æ—¶é‡æ–°åˆå§‹åŒ–
    if retriever is None or generator is None or current_provider != provider:
        init_result = init_components(provider)
        if "å¤±è´¥" in init_result:
            return init_result, "", ""
    
    try:
        # æ£€ç´¢å†å²èƒŒæ™¯
        retrieval_result = await retriever.retrieve(
            memoir_text, 
            top_k=10,
            use_llm_parsing=True
        )
        
        # æå–çš„ä¿¡æ¯
        context = retrieval_result.context
        extracted_info = f"""**æå–çš„æ—¶é—´**: {context.year or 'æœªè¯†åˆ«'}
**æå–çš„åœ°ç‚¹**: {context.location or 'æœªè¯†åˆ«'}
**å…³é”®è¯**: {', '.join(context.keywords) if context.keywords else 'æ— '}
**ç”Ÿæˆçš„æŸ¥è¯¢**: {retrieval_result.query}"""
        
        # æ£€ç´¢ç»“æœ
        retrieval_info = f"""**æ‰¾åˆ°å®ä½“**: {len(retrieval_result.entities)} ä¸ª
**æ‰¾åˆ°å…³ç³»**: {len(retrieval_result.relationships)} ä¸ª
**ç¤¾åŒºæŠ¥å‘Š**: {len(retrieval_result.communities)} ä¸ª
**ç›¸å…³æ–‡æœ¬**: {len(retrieval_result.text_units)} æ®µ"""
        
        if retrieval_result.entities:
            retrieval_info += "\n\n**ä¸»è¦å®ä½“**:\n"
            for entity in retrieval_result.entities[:5]:
                retrieval_info += f"- {entity.get('name', 'æœªçŸ¥')}: {entity.get('description', '')[:100]}...\n"
        
        # ç”Ÿæˆå†å²èƒŒæ™¯
        gen_result = await generator.generate(
            memoir_text=memoir_text,
            retrieval_result=retrieval_result,
            temperature=temperature,
        )
        
        return gen_result.content, extracted_info, retrieval_info
        
    except Exception as e:
        return f"å¤„ç†å¤±è´¥: {str(e)}", "", ""


def process_memoir(
    memoir_text: str,
    provider: str,
    style: str,
    temperature: float,
) -> Tuple[str, str, str]:
    """å¤„ç†å›å¿†å½•ï¼ˆåŒæ­¥åŒ…è£…ï¼‰"""
    return asyncio.run(process_memoir_async(memoir_text, provider, style, temperature))


async def compare_providers_async(
    memoir_text: str,
    selected_providers: List[str],
    temperature: float,
) -> str:
    """
    ä½¿ç”¨å¤šä¸ªLLMå¯¹æ¯”ç”Ÿæˆ
    """
    if not memoir_text.strip():
        return "è¯·è¾“å…¥å›å¿†å½•æ–‡æœ¬"
    
    if not selected_providers:
        return "è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªLLMæä¾›å•†"
    
    try:
        # åˆ›å»ºå¤šLLMè·¯ç”±å™¨
        router = LLMRouter()
        
        # åˆ›å»ºç”Ÿæˆå™¨
        gen = LiteraryGenerator(llm_router=router)
        
        # åˆ›å»ºæ£€ç´¢å™¨å¹¶æ£€ç´¢
        ret = MemoirRetriever()
        retrieval_result = ret.retrieve_sync(memoir_text, top_k=10)
        
        # å¹¶è¡Œç”Ÿæˆ
        multi_result = await gen.generate_parallel(
            memoir_text=memoir_text,
            retrieval_result=retrieval_result,
            providers=selected_providers,
            temperature=temperature,
        )
        
        # æ ¼å¼åŒ–ç»“æœ
        output = []
        for provider_name, result in multi_result.results.items():
            output.append(f"## {provider_name.upper()} ({result.model})\n")
            output.append(result.content)
            output.append("\n---\n")
        
        for provider_name, error in multi_result.errors.items():
            output.append(f"## {provider_name.upper()} - é”™è¯¯\n")
            output.append(f"âŒ {error}\n---\n")
        
        return "\n".join(output)
        
    except Exception as e:
        return f"å¯¹æ¯”ç”Ÿæˆå¤±è´¥: {str(e)}"


def compare_providers(
    memoir_text: str,
    selected_providers: List[str],
    temperature: float,
) -> str:
    """å¯¹æ¯”ç”Ÿæˆï¼ˆåŒæ­¥åŒ…è£…ï¼‰"""
    return asyncio.run(compare_providers_async(memoir_text, selected_providers, temperature))


def build_index(llm_provider: str) -> str:
    """æ„å»ºçŸ¥è¯†å›¾è°±ç´¢å¼•"""
    try:
        builder = GraphBuilder(llm_provider=llm_provider)
        result = builder.build_index_sync()
        
        if result.success:
            stats = builder.get_index_stats()
            return f"""âœ… ç´¢å¼•æ„å»ºæˆåŠŸï¼

**ç»Ÿè®¡ä¿¡æ¯**:
- è¾“å…¥æ–‡ä»¶: {stats['input_files']} ä¸ª
- å®ä½“æ•°é‡: {stats['entities']} ä¸ª
- å…³ç³»æ•°é‡: {stats['relationships']} ä¸ª
- ç¤¾åŒºæ•°é‡: {stats['communities']} ä¸ª

è¾“å‡ºç›®å½•: {result.output_dir}"""
        else:
            return f"âŒ ç´¢å¼•æ„å»ºå¤±è´¥: {result.message}"
            
    except Exception as e:
        return f"âŒ ç´¢å¼•æ„å»ºå¼‚å¸¸: {str(e)}"


def get_index_status() -> str:
    """è·å–ç´¢å¼•çŠ¶æ€"""
    try:
        builder = GraphBuilder()
        stats = builder.get_index_stats()
        
        if stats["indexed"]:
            return f"""âœ… ç´¢å¼•å·²å°±ç»ª

**ç»Ÿè®¡ä¿¡æ¯**:
- è¾“å…¥æ–‡ä»¶: {stats['input_files']} ä¸ª
- å®ä½“æ•°é‡: {stats['entities']} ä¸ª
- å…³ç³»æ•°é‡: {stats['relationships']} ä¸ª
- ç¤¾åŒºæ•°é‡: {stats['communities']} ä¸ª"""
        else:
            return f"""âš ï¸ ç´¢å¼•æœªæ„å»º

è¾“å…¥ç›®å½•ä¸­æœ‰ {stats['input_files']} ä¸ªæ–‡ä»¶ç­‰å¾…å¤„ç†ã€‚
è¯·ç‚¹å‡»"æ„å»ºç´¢å¼•"æŒ‰é’®å¼€å§‹æ„å»ºã€‚"""
            
    except Exception as e:
        return f"âŒ è·å–çŠ¶æ€å¤±è´¥: {str(e)}"


def create_ui():
    """åˆ›å»ºGradioç•Œé¢"""
    
    # è·å–å¯ç”¨çš„LLMæä¾›å•†
    available = get_available_providers()
    available_providers = [k for k, v in available.items() if v]
    if not available_providers:
        available_providers = ["deepseek"]  # é»˜è®¤
    
    with gr.Blocks(
        title="è®°å¿†å›¾è°± - å†å²èƒŒæ™¯æ³¨å…¥ç³»ç»Ÿ",
        theme=gr.themes.Soft(
            primary_hue="blue",
            secondary_hue="gray",
        ),
        css="""
        .main-title { text-align: center; margin-bottom: 20px; }
        .output-box { min-height: 200px; }
        """
    ) as app:
        
        gr.Markdown(
            """
            # ğŸ­ è®°å¿†å›¾è°±
            ### åŸºäºRAGä¸çŸ¥è¯†å›¾è°±çš„ä¸ªäººå›å¿†å½•å†å²èƒŒæ™¯è‡ªåŠ¨æ³¨å…¥ç³»ç»Ÿ
            
            è¾“å…¥æ‚¨çš„å›å¿†å½•ç‰‡æ®µï¼Œç³»ç»Ÿå°†è‡ªåŠ¨æ£€ç´¢ç›¸å…³å†å²èƒŒæ™¯ï¼Œå¹¶ç”Ÿæˆå…·æœ‰æ–‡å­¦æ€§çš„æè¿°æ–‡æœ¬ã€‚
            """,
            elem_classes=["main-title"]
        )
        
        with gr.Tabs():
            # ä¸»åŠŸèƒ½æ ‡ç­¾é¡µ
            with gr.TabItem("ğŸ“ ç”Ÿæˆå†å²èƒŒæ™¯"):
                with gr.Row():
                    with gr.Column(scale=1):
                        memoir_input = gr.Textbox(
                            label="å›å¿†å½•ç‰‡æ®µ",
                            placeholder="è¾“å…¥æ‚¨çš„å›å¿†å½•æ–‡æœ¬ï¼Œä¾‹å¦‚ï¼š\n1988å¹´å¤å¤©ï¼Œæˆ‘ä»å¤§å­¦æ¯•ä¸šï¼Œæ€€æ£ç€æ¢¦æƒ³æ¥åˆ°äº†æ·±åœ³...",
                            lines=8,
                        )
                        
                        with gr.Row():
                            provider_select = gr.Dropdown(
                                choices=available_providers,
                                value=available_providers[0] if available_providers else None,
                                label="LLM æ¨¡å‹",
                            )
                            style_select = gr.Dropdown(
                                choices=list(PromptTemplates.list_styles().keys()),
                                value="standard",
                                label="å†™ä½œé£æ ¼",
                            )
                        
                        temperature_slider = gr.Slider(
                            minimum=0.1,
                            maximum=1.0,
                            value=0.7,
                            step=0.1,
                            label="åˆ›æ„åº¦ (Temperature)",
                        )
                        
                        generate_btn = gr.Button("ğŸš€ ç”Ÿæˆå†å²èƒŒæ™¯", variant="primary")
                    
                    with gr.Column(scale=1):
                        output_text = gr.Textbox(
                            label="ç”Ÿæˆçš„å†å²èƒŒæ™¯",
                            lines=10,
                            elem_classes=["output-box"],
                        )
                        
                        with gr.Accordion("ğŸ“Š è¯¦ç»†ä¿¡æ¯", open=False):
                            extracted_info = gr.Markdown(label="æå–çš„ä¿¡æ¯")
                            retrieval_info = gr.Markdown(label="æ£€ç´¢ç»“æœ")
                
                generate_btn.click(
                    fn=process_memoir,
                    inputs=[memoir_input, provider_select, style_select, temperature_slider],
                    outputs=[output_text, extracted_info, retrieval_info],
                )
            
            # å¤šæ¨¡å‹å¯¹æ¯”æ ‡ç­¾é¡µ
            with gr.TabItem("ğŸ”„ å¤šæ¨¡å‹å¯¹æ¯”"):
                gr.Markdown("ä½¿ç”¨å¤šä¸ªLLMåŒæ—¶ç”Ÿæˆï¼Œå¯¹æ¯”ä¸åŒæ¨¡å‹çš„è¾“å‡ºæ•ˆæœã€‚")
                
                with gr.Row():
                    with gr.Column(scale=1):
                        compare_input = gr.Textbox(
                            label="å›å¿†å½•ç‰‡æ®µ",
                            placeholder="è¾“å…¥å›å¿†å½•æ–‡æœ¬...",
                            lines=6,
                        )
                        
                        providers_checkbox = gr.CheckboxGroup(
                            choices=available_providers,
                            value=available_providers[:2] if len(available_providers) >= 2 else available_providers,
                            label="é€‰æ‹©è¦å¯¹æ¯”çš„æ¨¡å‹",
                        )
                        
                        compare_temp = gr.Slider(
                            minimum=0.1,
                            maximum=1.0,
                            value=0.7,
                            step=0.1,
                            label="åˆ›æ„åº¦",
                        )
                        
                        compare_btn = gr.Button("ğŸ”„ å¼€å§‹å¯¹æ¯”", variant="primary")
                    
                    with gr.Column(scale=1):
                        compare_output = gr.Markdown(label="å¯¹æ¯”ç»“æœ")
                
                compare_btn.click(
                    fn=compare_providers,
                    inputs=[compare_input, providers_checkbox, compare_temp],
                    outputs=[compare_output],
                )
            
            # ç´¢å¼•ç®¡ç†æ ‡ç­¾é¡µ
            with gr.TabItem("âš™ï¸ ç´¢å¼•ç®¡ç†"):
                gr.Markdown("""
                ## ğŸ“Š çŸ¥è¯†å›¾è°±ç´¢å¼•ç®¡ç†
                
                ç´¢å¼•æ„å»ºä¼šä½¿ç”¨ LLM ä»å†å²äº‹ä»¶æ•°æ®ä¸­æå–å®ä½“å’Œå…³ç³»ï¼Œæ„å»ºçŸ¥è¯†å›¾è°±ã€‚
                
                **æ”¯æŒçš„ LLM æ¨¡å‹ï¼š**
                | æä¾›å•† | ç¯å¢ƒå˜é‡ | é»˜è®¤æ¨¡å‹ |
                |-------|----------|---------|
                | Gemini | `GOOGLE_API_KEY` | gemini-2.0-flash |
                | DeepSeek | `DEEPSEEK_API_KEY` | deepseek-chat |
                | Qwen | `QWEN_API_KEY` | qwen-plus |
                | OpenAI | `OPENAI_API_KEY` | gpt-4o-mini |
                | æ··å…ƒ | `HUNYUAN_API_KEY` | hunyuan-lite |
                
                **æ³¨æ„**ï¼šé¦–æ¬¡æ„å»ºç´¢å¼•éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼Œä¼šäº§ç”Ÿä¸€å®šçš„ API è´¹ç”¨ã€‚
                """)
                
                with gr.Row():
                    with gr.Column():
                        index_status = gr.Markdown(value=get_index_status())
                        
                        refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°çŠ¶æ€")
                        refresh_btn.click(fn=get_index_status, outputs=[index_status])
                    
                    with gr.Column():
                        # å®Œæ•´çš„æ¨¡å‹é€‰æ‹©åˆ—è¡¨
                        all_providers = ["gemini", "deepseek", "qwen", "openai", "hunyuan"]
                        default_provider = available_providers[0] if available_providers else "gemini"
                        
                        index_provider = gr.Dropdown(
                            choices=all_providers,
                            value=default_provider,
                            label="æ„å»ºç´¢å¼•ä½¿ç”¨çš„ LLM",
                            info="é€‰æ‹©ç”¨äºæ„å»ºçŸ¥è¯†å›¾è°±çš„è¯­è¨€æ¨¡å‹",
                        )
                        
                        build_btn = gr.Button("ğŸ”¨ æ„å»ºç´¢å¼•", variant="primary")
                        build_output = gr.Markdown()
                        
                        build_btn.click(
                            fn=build_index,
                            inputs=[index_provider],
                            outputs=[build_output],
                        )
            
            # ä½¿ç”¨è¯´æ˜æ ‡ç­¾é¡µ
            with gr.TabItem("ğŸ“– ä½¿ç”¨è¯´æ˜"):
                gr.Markdown("""
                ## ä½¿ç”¨æŒ‡å—
                
                ### 1. å‡†å¤‡å·¥ä½œ
                
                1. åœ¨ `.env` æ–‡ä»¶ä¸­é…ç½®æ‚¨çš„ LLM API å¯†é’¥
                2. å°†å†å²äº‹ä»¶æ•°æ®æ”¾å…¥ `data/input/` ç›®å½•
                3. æ„å»ºçŸ¥è¯†å›¾è°±ç´¢å¼•
                
                ### 2. ç”Ÿæˆå†å²èƒŒæ™¯
                
                1. åœ¨"ç”Ÿæˆå†å²èƒŒæ™¯"æ ‡ç­¾é¡µè¾“å…¥å›å¿†å½•ç‰‡æ®µ
                2. é€‰æ‹© LLM æ¨¡å‹å’Œå†™ä½œé£æ ¼
                3. è°ƒæ•´åˆ›æ„åº¦å‚æ•°
                4. ç‚¹å‡»"ç”Ÿæˆå†å²èƒŒæ™¯"æŒ‰é’®
                
                ### 3. å¤šæ¨¡å‹å¯¹æ¯”
                
                1. åœ¨"å¤šæ¨¡å‹å¯¹æ¯”"æ ‡ç­¾é¡µè¾“å…¥å›å¿†å½•ç‰‡æ®µ
                2. é€‰æ‹©å¤šä¸ª LLM æ¨¡å‹
                3. ç‚¹å‡»"å¼€å§‹å¯¹æ¯”"æŸ¥çœ‹ä¸åŒæ¨¡å‹çš„è¾“å‡º
                
                ### 4. å†™ä½œé£æ ¼è¯´æ˜
                
                - **standard**: æ ‡å‡†é£æ ¼ï¼Œå¹³è¡¡çš„æ–‡å­¦æ€§æè¿°
                - **nostalgic**: æ€€æ—§é£æ ¼ï¼Œæ¸©æš–å›å¿†çš„ç¬”è°ƒ
                - **narrative**: å™äº‹èåˆï¼Œä¸ä¸ªäººæ•…äº‹æ·±åº¦äº¤ç»‡
                - **informative**: ç®€æ´ä¿¡æ¯ï¼Œé‡ç‚¹çªå‡ºçš„èƒŒæ™¯ä»‹ç»
                - **conversational**: å¯¹è¯é£æ ¼ï¼Œåƒè®²æ•…äº‹ä¸€æ ·äº²åˆ‡
                
                ### 5. æ”¯æŒçš„ LLM æ¨¡å‹
                
                - Deepseek
                - Qwen (é€šä¹‰åƒé—®)
                - Hunyuan (è…¾è®¯æ··å…ƒ)
                - Google Gemini
                - OpenAI GPT
                """)
        
        gr.Markdown(
            """
            ---
            **è®°å¿†å›¾è°±** - è®©å†å²ä¸ºæ‚¨çš„å›å¿†å¢æ·»åšåº¦ | Capstone Project 2026
            """
        )
    
    return app


if __name__ == "__main__":
    app = create_ui()
    app.launch(
        server_name=settings.app_host,
        server_port=settings.app_port,
        share=False,
    )
